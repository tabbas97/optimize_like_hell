{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu121\n"
     ]
    }
   ],
   "source": [
    "# Using version 2.3.0 of torch on Python 3.10 as of May 26 2024. \n",
    "\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXACT VERSION USED : 2.3.0+cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SampleModel(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Model - A simple conv neural network with 5 hidden layers\n",
    "\n",
    "class SampleModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SampleModel, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, 3, 1)\n",
    "        self.conv3 = torch.nn.Conv2d(64, 128, 3, 1)\n",
    "        self.conv4 = torch.nn.Conv2d(128, 256, 3, 1)\n",
    "        self.conv5 = torch.nn.Conv2d(256, 512, 3, 1)\n",
    "        self.fc1 = torch.nn.Linear(512, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.conv1(x))\n",
    "        x = torch.nn.functional.relu(self.conv2(x))\n",
    "        x = torch.nn.functional.relu(self.conv3(x))\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "        x = torch.nn.functional.relu(self.conv4(x))\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "        x = torch.nn.functional.relu(self.conv5(x))\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = SampleModel()\n",
    "\n",
    "model.cuda() # Move model to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %conv1 : [num_users=1] = call_module[target=conv1](args = (%x,), kwargs = {})\n",
      "    %relu : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%conv1,), kwargs = {inplace: False})\n",
      "    %conv2 : [num_users=1] = call_module[target=conv2](args = (%relu,), kwargs = {})\n",
      "    %relu_1 : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%conv2,), kwargs = {inplace: False})\n",
      "    %conv3 : [num_users=1] = call_module[target=conv3](args = (%relu_1,), kwargs = {})\n",
      "    %relu_2 : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%conv3,), kwargs = {inplace: False})\n",
      "    %max_pool2d : [num_users=1] = call_function[target=torch.nn.functional.max_pool2d](args = (%relu_2, 2), kwargs = {stride: None, padding: 0, dilation: 1, ceil_mode: False, return_indices: False})\n",
      "    %conv4 : [num_users=1] = call_module[target=conv4](args = (%max_pool2d,), kwargs = {})\n",
      "    %relu_3 : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%conv4,), kwargs = {inplace: False})\n",
      "    %max_pool2d_1 : [num_users=1] = call_function[target=torch.nn.functional.max_pool2d](args = (%relu_3, 2), kwargs = {stride: None, padding: 0, dilation: 1, ceil_mode: False, return_indices: False})\n",
      "    %conv5 : [num_users=1] = call_module[target=conv5](args = (%max_pool2d_1,), kwargs = {})\n",
      "    %relu_4 : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%conv5,), kwargs = {inplace: False})\n",
      "    %max_pool2d_2 : [num_users=1] = call_function[target=torch.nn.functional.max_pool2d](args = (%relu_4, 2), kwargs = {stride: None, padding: 0, dilation: 1, ceil_mode: False, return_indices: False})\n",
      "    %flatten : [num_users=1] = call_function[target=torch.flatten](args = (%max_pool2d_2, 1), kwargs = {})\n",
      "    %fc1 : [num_users=1] = call_module[target=fc1](args = (%flatten,), kwargs = {})\n",
      "    %relu_5 : [num_users=1] = call_function[target=torch.nn.functional.relu](args = (%fc1,), kwargs = {inplace: False})\n",
      "    %fc2 : [num_users=1] = call_module[target=fc2](args = (%relu_5,), kwargs = {})\n",
      "    return fc2\n"
     ]
    }
   ],
   "source": [
    "# We follow fx graph\n",
    "\n",
    "from torch.fx import symbolic_trace\n",
    "\n",
    "symbolic_traced_model = symbolic_trace(model)\n",
    "\n",
    "print(symbolic_traced_model.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    conv1 = self.conv1(x);  x = None\n",
      "    relu = torch.nn.functional.relu(conv1, inplace = False);  conv1 = None\n",
      "    conv2 = self.conv2(relu);  relu = None\n",
      "    relu_1 = torch.nn.functional.relu(conv2, inplace = False);  conv2 = None\n",
      "    conv3 = self.conv3(relu_1);  relu_1 = None\n",
      "    relu_2 = torch.nn.functional.relu(conv3, inplace = False);  conv3 = None\n",
      "    max_pool2d = torch.nn.functional.max_pool2d(relu_2, 2, stride = None, padding = 0, dilation = 1, ceil_mode = False, return_indices = False);  relu_2 = None\n",
      "    conv4 = self.conv4(max_pool2d);  max_pool2d = None\n",
      "    relu_3 = torch.nn.functional.relu(conv4, inplace = False);  conv4 = None\n",
      "    max_pool2d_1 = torch.nn.functional.max_pool2d(relu_3, 2, stride = None, padding = 0, dilation = 1, ceil_mode = False, return_indices = False);  relu_3 = None\n",
      "    conv5 = self.conv5(max_pool2d_1);  max_pool2d_1 = None\n",
      "    relu_4 = torch.nn.functional.relu(conv5, inplace = False);  conv5 = None\n",
      "    max_pool2d_2 = torch.nn.functional.max_pool2d(relu_4, 2, stride = None, padding = 0, dilation = 1, ceil_mode = False, return_indices = False);  relu_4 = None\n",
      "    flatten = torch.flatten(max_pool2d_2, 1);  max_pool2d_2 = None\n",
      "    fc1 = self.fc1(flatten);  flatten = None\n",
      "    relu_5 = torch.nn.functional.relu(fc1, inplace = False);  fc1 = None\n",
      "    fc2 = self.fc2(relu_5);  relu_5 = None\n",
      "    return fc2\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(symbolic_traced_model.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-26 23:48:29 57781:57781 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-05-26 23:48:29 57781:57781 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-26 23:48:29 57781:57781 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "# What's happening on eager execution\n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "input = torch.randn(100, 1, 28, 28)\n",
    "input = input.cuda()\n",
    "\n",
    "with profile(activities=[\n",
    "    ProfilerActivity.CUDA, # Will only record the time spent on GPU\n",
    "    ProfilerActivity.CPU   # Will only record the time spent on CPU. Need both to get the complete picture\n",
    "    ], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(input)\n",
    "        \n",
    "prof.export_chrome_trace(\"trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-26 23:48:42 57781:57781 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-05-26 23:48:42 57781:57781 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-26 23:48:42 57781:57781 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "# We will use Torch.compile to compile the model\n",
    "\n",
    "compiled_model = torch.compile(\n",
    "    model=model,\n",
    "    fullgraph=True, # Ideally we want the complete graph on device. Default is set to False\n",
    "                    # In case the full graph is not compilable, we can set it to False.\n",
    "    )\n",
    "\n",
    "with profile(activities=[\n",
    "    ProfilerActivity.CUDA, # Will only record the time spent on GPU\n",
    "    ProfilerActivity.CPU   # Will only record the time spent on CPU. Need both to get the complete picture\n",
    "    ], record_shapes=True) as compiled_prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        compiled_model(input)\n",
    "        \n",
    "compiled_prof.export_chrome_trace(\"compiled_trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up use of tensor cores\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-26 23:49:29 57781:57781 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-05-26 23:49:29 57781:57781 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-26 23:49:29 57781:57781 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "# Compile model again\n",
    "new_compiled_model = torch.compile(\n",
    "    model=model,\n",
    "    fullgraph=True, # Ideally we want the complete graph on device. Default is set to False\n",
    "                    # In case the full graph is not compilable, we can set it to False.\n",
    "    )\n",
    "\n",
    "with profile(activities=[\n",
    "    ProfilerActivity.CUDA, # Will only record the time spent on GPU\n",
    "    ProfilerActivity.CPU   # Will only record the time spent on CPU. Need both to get the complete picture\n",
    "    ], record_shapes=True) as new_compiled_prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        new_compiled_model(input)\n",
    "        \n",
    "new_compiled_prof.export_chrome_trace(\"new_compiled_trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-27 02:11:43 57781:57781 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-05-27 02:11:43 57781:57781 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-27 02:11:43 57781:57781 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "# Switching to FP16\n",
    "torch.set_default_dtype(torch.float16)\n",
    "\n",
    "# Model to FP16\n",
    "fp_16_base_model = model.half()\n",
    "input_fp16 = input.half().cuda()\n",
    "\n",
    "# Compile model again\n",
    "fp16_compiled_model = torch.compile(\n",
    "    model=fp_16_base_model,\n",
    "    fullgraph=True, # Ideally we want the complete graph on device. Default is set to False\n",
    "                    # In case the full graph is not compilable, we can set it to False.\n",
    "    )\n",
    "\n",
    "with profile(activities=[\n",
    "    ProfilerActivity.CUDA, # Will only record the time spent on GPU\n",
    "    ProfilerActivity.CPU   # Will only record the time spent on CPU. Need both to get the complete picture\n",
    "    ], record_shapes=True) as fp16_compiled_prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        fp16_compiled_model(input_fp16)\n",
    "        \n",
    "fp16_compiled_prof.export_chrome_trace(\"fp16_compiled_trace.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FP16 cuts down processing time in half\n",
    "\n",
    "TODO : Need to investigate the effect of FP32 in RTX 20 series Tensor cores. The 2nd gen Tensor Cores. \n",
    "Preliminary assessment : Leaving the dtype on FP32 effectively does not use the Tensor Cores. We would need to drop into FP16, INT8, INT4, INT1.\n",
    "\n",
    "Note: Not all Tensor cores are the same. The 40 series has 4th gen Tensor Cores. Blackwell is on 5th gen. Refer to this sheet for the capabilities of each of the generation of Tensor Cores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to this blog for more information on the GPU family vs types supported\n",
    "\n",
    "https://bruce-lee-ly.medium.com/nvidia-tensor-core-preliminary-exploration-10618787615a\n",
    "\n",
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaving in a cell to be run on the latest gen on L4 GPU - Hopper Architecture\n",
    "\n",
    "# Hopper natively supports TF32 on Tensor Cores. That should give the next big boost in performance.\n",
    "# We will see this differnce in profiling on previous profiled traces.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result of Compiling\n",
    "\n",
    "The major reason to compile is:\n",
    "1. To avoid the need to sync any of the operations back to the CPU\n",
    "2. Automatically introduces what you could consider true async operation at the device(GPU) level. The graph would have a separate location for the CUDA memsync to happen back to the CPU. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model_inference time for all the traces\n",
    "\n",
    "print(\"Eager Execution Time: \", prof.key_averages().table(row_limit=1))\n",
    "print(\"Compiled Execution Time: \", compiled_prof.key_averages().table(row_limit=1))\n",
    "print(\"New Compiled Execution Time: \", new_compiled_prof.key_averages().table(row_limit=1))\n",
    "print(\"FP16 Compiled Execution Time: \", fp16_compiled_prof.key_averages().table(row_limit=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Times - RTX 2070 Super\n",
    "\n",
    "Note: The GPU is also running 3 displays and might have effect on these numbers\n",
    "\n",
    "| Mode | Self CPU Total Time | Self CUDA Total Time |\n",
    "|------|---------------------|----------------------|\n",
    "|Eager Execution|128.710ms|60.790ms|\n",
    "|Basic Compile  |6.446ms|10.682ms|\n",
    "|Compiled Tensor Core Enable (RTX2070 Super)| 7.200ms | 10.224ms |\n",
    "|Compiled Tensor Core FP16 (RTX2070 Super)| 3.584ms | 2.612ms |\n",
    "\n",
    "Eager Execution Time:\n",
    "\n",
    "|               Name |   Self CPU % |     Self CPU |  CPU total % |    CPU total | CPU time avg |    Self CUDA |  Self CUDA %  |  CUDA total | CUDA time avg |   # of Calls |\n",
    "|--------------------|--------------|--------------|--------------|--------------|--------------|--------------|---------------|-------------|---------------|--------------|\n",
    "|    eager_execution |        0.65% |    842.000us |      100.00% |    128.704ms |    128.704ms |      0.000us |        0.00%  |     3.364ms |      3.364ms  |           1  |\n",
    "\n",
    "Compiled Execution Time:\n",
    "\n",
    "|               Name |   Self CPU % |     Self CPU |  CPU total % |    CPU total | CPU time avg |    Self CUDA |  Self CUDA %  |  CUDA total | CUDA time avg |   # of Calls |\n",
    "|--------------------|--------------|--------------|--------------|--------------|--------------|--------------|---------------|-------------|---------------|--------------|\n",
    "|    model_inference |        2.79% |    180.000us |       48.74% |      3.142ms |      3.142ms |      0.000us |        0.00%  |     4.293ms |      4.293ms  |           1  |\n",
    "\n",
    "New Compiled Execution Time:\n",
    "\n",
    "|               Name |   Self CPU % |     Self CPU |  CPU total % |    CPU total | CPU time avg |    Self CUDA |  Self CUDA %  |  CUDA total | CUDA time avg |   # of Calls |\n",
    "|--------------------|--------------|--------------|--------------|--------------|--------------|--------------|---------------|-------------|---------------|--------------|\n",
    "|    model_inference |        4.65% |    335.000us |       53.57% |      3.857ms |      3.857ms |      0.000us |        0.00%  |     4.283ms |      4.283ms  |           1  |\n",
    "\n",
    "FP16 Compiled Execution Time:\n",
    "\n",
    "|               Name |   Self CPU % |     Self CPU |  CPU total % |    CPU total | CPU time avg |    Self CUDA |  Self CUDA %  |  CUDA total | CUDA time avg |   # of Calls  |\n",
    "|--------------------|--------------|--------------|--------------|--------------|--------------|--------------|---------------|-------------|---------------|--------------|\n",
    "|    model_inference |        5.47% |    196.000us |       92.97% |      3.332ms |      3.332ms |      0.000us |        0.00%  |     1.180ms |      1.180ms  |           1   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA Graph Mode Execution\n",
    "\n",
    "CUDA graph mode execution offers a more streamlined graph operation onboard the GPU. This is turned off by default in Pytorch for compatibility and memory reasons.\n",
    "CUDA graph mode is known to consume slightly higher RAM than the normal compiled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA Graph mode execution with FP16\n",
    "\n",
    "fp16_cuda_graph = torch.compile(\n",
    "    model=fp_16_base_model,\n",
    "    fullgraph=True, # Ideally we want the complete graph on device. Default is set to False\n",
    "                    # In case the full graph is not compilable, we can set it to False.\n",
    "    mode=\"reduce-overhead\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-27 02:17:20 57781:57781 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-05-27 02:17:20 57781:57781 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-27 02:17:20 57781:57781 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    with profile(activities=[\n",
    "        ProfilerActivity.CUDA, # Will only record the time spent on GPU\n",
    "        ProfilerActivity.CPU   # Will only record the time spent on CPU. Need both to get the complete picture\n",
    "        ], record_shapes=True) as fp16_cuda_graph_prof:\n",
    "        with record_function(\"model_inference\"):\n",
    "            fp16_cuda_graph(input_fp16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fp16_cuda_graph_prof.export_chrome_trace(\"fp16_cuda_graph_trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Graph Mode Execution Time:  -------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference        11.87%     340.000us        68.89%       1.973ms       1.973ms       0.000us         0.00%       1.149ms       1.149ms             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.864ms\n",
      "Self CUDA time total: 2.568ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Graph Mode Execution Time: \", fp16_cuda_graph_prof.key_averages().table(row_limit=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not necessarily affect the overall time but does reduce the device(GPU) side time. The effects of this would be observed better with a pipeline of calls being made to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "Quantization is the next big speedup that we can observe. The effects of quantization are generally seen at higher model sizes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
